{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 15 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   subject   18539 non-null  object\n",
      " 1   body      19998 non-null  object\n",
      " 2   answer    19996 non-null  object\n",
      " 3   type      20000 non-null  object\n",
      " 4   queue     20000 non-null  object\n",
      " 5   priority  20000 non-null  object\n",
      " 6   language  20000 non-null  object\n",
      " 7   tag_1     20000 non-null  object\n",
      " 8   tag_2     19954 non-null  object\n",
      " 9   tag_3     19905 non-null  object\n",
      " 10  tag_4     18461 non-null  object\n",
      " 11  tag_5     13091 non-null  object\n",
      " 12  tag_6     7351 non-null   object\n",
      " 13  tag_7     3928 non-null   object\n",
      " 14  tag_8     1907 non-null   object\n",
      "dtypes: object(15)\n",
      "memory usage: 2.3+ MB\n",
      "None\n",
      "                                             subject  \\\n",
      "0  Unvorhergesehener Absturz der Datenanalyse-Pla...   \n",
      "1                           Customer Support Inquiry   \n",
      "2                      Data Analytics for Investment   \n",
      "3                 Krankenhaus-Dienstleistung-Problem   \n",
      "4                                           Security   \n",
      "\n",
      "                                                body  \\\n",
      "0  Die Datenanalyse-Plattform brach unerwartet ab...   \n",
      "1  Seeking information on digital strategies that...   \n",
      "2  I am contacting you to request information on ...   \n",
      "3  Ein Medien-Daten-Sperrverhalten trat aufgrund ...   \n",
      "4  Dear Customer Support, I am reaching out to in...   \n",
      "\n",
      "                                              answer      type  \\\n",
      "0  Ich werde Ihnen bei der Lösung des Problems he...  Incident   \n",
      "1  We offer a variety of digital strategies and s...   Request   \n",
      "2  I am here to assist you with data analytics to...   Request   \n",
      "3  Zurück zur E-Mail-Beschwerde über den Sperrver...  Incident   \n",
      "4  Dear [name], we take the security of medical d...   Request   \n",
      "\n",
      "              queue priority language      tag_1      tag_2       tag_3  \\\n",
      "0   General Inquiry      low       de      Crash  Technical         Bug   \n",
      "1  Customer Service   medium       en   Feedback      Sales          IT   \n",
      "2  Customer Service   medium       en  Technical    Product    Guidance   \n",
      "3  Customer Service     high       de   Security     Breach       Login   \n",
      "4  Customer Service   medium       en   Security   Customer  Compliance   \n",
      "\n",
      "           tag_4          tag_5       tag_6          tag_7 tag_8  \n",
      "0       Hardware     Resolution      Outage  Documentation   NaN  \n",
      "1   Tech Support            NaN         NaN            NaN   NaN  \n",
      "2  Documentation    Performance     Feature            NaN   NaN  \n",
      "3    Maintenance       Incident  Resolution       Feedback   NaN  \n",
      "4         Breach  Documentation    Guidance            NaN   NaN  \n",
      "subject      1461\n",
      "body            2\n",
      "answer          4\n",
      "type            0\n",
      "queue           0\n",
      "priority        0\n",
      "language        0\n",
      "tag_1           0\n",
      "tag_2          46\n",
      "tag_3          95\n",
      "tag_4        1539\n",
      "tag_5        6909\n",
      "tag_6       12649\n",
      "tag_7       16072\n",
      "tag_8       18093\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"dataset-tickets-multi-lang-4-20k.csv\")\n",
    "\n",
    "# Check columns, missing values, and sample data\n",
    "print(df.info())\n",
    "print(df.head())\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "# Load language models (English/German)\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "nlp_de = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "def clean_text(text, language=\"en\"):\n",
    "    # Handle missing/NaN values\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string if not already\n",
    "    text = str(text)\n",
    "    \n",
    "    # Remove URLs, special characters, numbers\n",
    "    text = re.sub(r'http\\S+|www\\S+|@\\w+|[^a-zA-ZäöüßÄÖÜ ]', '', text)\n",
    "    \n",
    "    # Lemmatize and remove stopwords\n",
    "    if language == \"en\":\n",
    "        doc = nlp_en(text)\n",
    "    else:\n",
    "        doc = nlp_de(text)\n",
    "    \n",
    "    tokens = [token.lemma_.lower() for token in doc if not token.is_stop]\n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of cleaned texts:\n",
      "                                                body  \\\n",
      "0  Die Datenanalyse-Plattform brach unerwartet ab...   \n",
      "1  Seeking information on digital strategies that...   \n",
      "2  I am contacting you to request information on ...   \n",
      "3  Ein Medien-Daten-Sperrverhalten trat aufgrund ...   \n",
      "4  Dear Customer Support, I am reaching out to in...   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0  datenanalyseplattform brechen unerwartet speic...  \n",
      "1  seek information digital strategy aid brand gr...  \n",
      "2  contact request information datum analytic too...  \n",
      "3  mediendatensperrverhalten treten aufgrund uner...  \n",
      "4  dear customer support reach inquire security p...  \n",
      "\n",
      "Cleaning Statistics:\n",
      "Total rows: 20000\n",
      "Rows with empty cleaned text: 2\n",
      "Average cleaned text length: 267.37 characters\n"
     ]
    }
   ],
   "source": [
    "# First, fill NaN values in body column\n",
    "df['body'] = df['body'].fillna(\"\")\n",
    "\n",
    "# Now apply cleaning based on language column\n",
    "df[\"cleaned_text\"] = df.apply(lambda row: clean_text(row[\"body\"], language=row[\"language\"]), axis=1)\n",
    "\n",
    "# Check the results\n",
    "print(\"\\nSample of cleaned texts:\")\n",
    "print(df[['body', 'cleaned_text']].head())\n",
    "\n",
    "# Print statistics about cleaning\n",
    "print(\"\\nCleaning Statistics:\")\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "print(f\"Rows with empty cleaned text: {(df['cleaned_text'] == '').sum()}\")\n",
    "print(f\"Average cleaned text length: {df['cleaned_text'].str.len().mean():.2f} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Combine tags into a list (ignore empty tags)\n",
    "tags = df.apply(lambda row: [t for t in row[[\"tag_1\", \"tag_2\",\"tag_3\", \"tag_4\", \"tag_5\", \"tag_6\", \"tag_7\", \"tag_8\"]].dropna()], axis=1)\n",
    "\n",
    "# Binarize tags\n",
    "mlb = MultiLabelBinarizer()\n",
    "tag_matrix = pd.DataFrame(mlb.fit_transform(tags), columns=mlb.classes_)\n",
    "\n",
    "# Merge with original data\n",
    "df = pd.concat([df, tag_matrix], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df[\"subject\"] + \" \" + df[\"cleaned_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode 'type' and 'queue'\n",
    "df = pd.get_dummies(df, columns=[\"type\", \"queue\"])\n",
    "\n",
    "# Map priority to ordinal values\n",
    "priority_map = {\"low\": 0, \"medium\": 1, \"high\": 2}\n",
    "df[\"priority\"] = df[\"priority\"].map(priority_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 1383)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Unvorhergesehener Absturz der Datenanalyse-Pla...\n",
       "1        Customer Support Inquiry seek information digi...\n",
       "2        Data Analytics for Investment contact request ...\n",
       "3        Krankenhaus-Dienstleistung-Problem mediendaten...\n",
       "4        Security dear customer support reach inquire s...\n",
       "                               ...                        \n",
       "19995    Assistance Needed for IFTTT Docker Integration...\n",
       "19996    Bitten um Unterstützung bei der Integration ge...\n",
       "19997                                                  NaN\n",
       "19998    Hilfe bei digitalen Strategie-Problemen qualit...\n",
       "19999    Optimierung Ihrer Datenanalyse-Plattform erlei...\n",
       "Name: text, Length: 20000, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0        datenanalyseplattform brechen unerwartet speic...\n",
       "1        seek information digital strategy aid brand gr...\n",
       "2        contact request information datum analytic too...\n",
       "3        mediendatensperrverhalten treten aufgrund uner...\n",
       "4        dear customer support reach inquire security p...\n",
       "                               ...                        \n",
       "19995    face integration problem ifttt docker cause di...\n",
       "19996    geehrt kundenservice integrationsunterstützung...\n",
       "19997    hello customer support inquire billing option ...\n",
       "19998    qualität digital strategiebearbeitungen negati...\n",
       "19999    geehrt customer supportteam schreiben erkunden...\n",
       "Name: cleaned_text, Length: 20000, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df[\"text\"])\n",
    "display(df[\"cleaned_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[0;32m      3\u001b[0m tfidf \u001b[38;5;241m=\u001b[39m TfidfVectorizer(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m text_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mtfidf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m X_text \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(text_embeddings\u001b[38;5;241m.\u001b[39mtoarray(), columns\u001b[38;5;241m=\u001b[39mtfidf\u001b[38;5;241m.\u001b[39mget_feature_names_out())\n",
      "File \u001b[1;32mc:\\Users\\keert\\OneDrive\\Desktop\\KEERTHIRAJ\\PROJECTS\\Ticketing-Chatbot\\ticketchatbot\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2104\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2098\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2099\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2100\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2101\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2102\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2103\u001b[0m )\n\u001b[1;32m-> 2104\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2106\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2107\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\keert\\OneDrive\\Desktop\\KEERTHIRAJ\\PROJECTS\\Ticketing-Chatbot\\ticketchatbot\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\keert\\OneDrive\\Desktop\\KEERTHIRAJ\\PROJECTS\\Ticketing-Chatbot\\ticketchatbot\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1376\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1368\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1369\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1370\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1371\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1372\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1373\u001b[0m             )\n\u001b[0;32m   1374\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1376\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1379\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\keert\\OneDrive\\Desktop\\KEERTHIRAJ\\PROJECTS\\Ticketing-Chatbot\\ticketchatbot\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1263\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1262\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1263\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1264\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1265\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32mc:\\Users\\keert\\OneDrive\\Desktop\\KEERTHIRAJ\\PROJECTS\\Ticketing-Chatbot\\ticketchatbot\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:99\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Chain together an optional series of text processing steps to go from\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;124;03ma single document to ngrams, with or without tokenizing or preprocessing.\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;124;03m    A sequence of tokens, possibly with pairs, triples, etc.\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decoder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 99\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m analyzer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    101\u001b[0m     doc \u001b[38;5;241m=\u001b[39m analyzer(doc)\n",
      "File \u001b[1;32mc:\\Users\\keert\\OneDrive\\Desktop\\KEERTHIRAJ\\PROJECTS\\Ticketing-Chatbot\\ticketchatbot\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:232\u001b[0m, in \u001b[0;36m_VectorizerMixin.decode\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m    229\u001b[0m     doc \u001b[38;5;241m=\u001b[39m doc\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode_error)\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m doc \u001b[38;5;129;01mis\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan:\n\u001b[1;32m--> 232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    233\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.nan is an invalid document, expected byte or unicode string.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    234\u001b[0m     )\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "\u001b[1;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=1000)\n",
    "text_embeddings = tfidf.fit_transform(df[\"text\"])\n",
    "X_text = pd.DataFrame(text_embeddings.toarray(), columns=tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maintenance_tags = [\"Hardware\", \"Crash\", \"Outage\", \"Security\"]\n",
    "df[\"requires_maintenance\"] = df[maintenance_tags].any(axis=1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([X_text, df[[\"priority\", \"type_Incident\", \"type_Request\", ...]]], axis=1)\n",
    "y = df[\"requires_maintenance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
